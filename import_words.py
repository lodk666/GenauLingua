import asyncio
import re
from sqlalchemy import select
from app.database.session import AsyncSessionLocal
from app.database.models import MasterWord, Category, WordCategory, CEFRLevel, Article, PartOfSpeech


async def parse_word_line(line: str) -> dict | None:
    """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–æ–∫—É —Ñ–æ—Ä–º–∞—Ç–∞ | —Å–ª–æ–≤–æ | –ø–µ—Ä–µ–≤–æ–¥ |"""
    # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ —Ä–∞–∑–¥–µ–ª—è–µ–º –ø–æ |
    parts = [p.strip() for p in line.split('|') if p.strip()]

    if len(parts) < 2:
        return None

    word_raw = parts[0]
    translation_raw = parts[1]

    # –£–±–∏—Ä–∞–µ–º –ø–æ–º–µ—Ç–∫–∏ –∏–∑ –ø–µ—Ä–µ–≤–æ–¥–∞: *(–Ω–∞—Ä–µ—á–∏–µ)*, *(–ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ)* –∏ —Ç.–¥.
    translation = re.sub(r'\s*\*\([^)]+\)\*', '', translation_raw).strip()

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞—Ä—Ç–∏–∫–ª—å –∏ —á–∞—Å—Ç—å —Ä–µ—á–∏
    article = Article.NONE
    pos = PartOfSpeech.OTHER
    lemma = word_raw

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä—Ç–∏–∫–ª–∏
    if word_raw.startswith('der '):
        article = Article.DER
        lemma = word_raw[4:]
        pos = PartOfSpeech.NOUN
    elif word_raw.startswith('die '):
        article = Article.DIE
        lemma = word_raw[4:]
        pos = PartOfSpeech.NOUN
    elif word_raw.startswith('das '):
        article = Article.DAS
        lemma = word_raw[4:]
        pos = PartOfSpeech.NOUN
    else:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —á–∞—Å—Ç—å —Ä–µ—á–∏ –±–µ–∑ –∞—Ä—Ç–∏–∫–ª—è
        if '*(–ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ)*' in translation_raw or '*(–ø—Ä–∏—á.)*' in translation_raw:
            pos = PartOfSpeech.ADJECTIVE
        elif '*(–Ω–∞—Ä–µ—á–∏–µ)*' in translation_raw:
            pos = PartOfSpeech.ADVERB
        elif lemma.endswith(('en', 'n')) and not lemma.endswith(('nen', 'len', 'ren', 'sen')):
            # –í–µ—Ä–æ—è—Ç–Ω–æ –≥–ª–∞–≥–æ–ª (–æ–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ -en)
            # –ù–æ –∏—Å–∫–ª—é—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–∏–ø–∞ "Kissen"
            if not word_raw[0].isupper():  # –ì–ª–∞–≥–æ–ª—ã —Å –º–∞–ª–µ–Ω—å–∫–æ–π –±—É–∫–≤—ã
                pos = PartOfSpeech.VERB
        elif '*(–º–µ—Å—Ç–æ–∏–º–µ–Ω–∏–µ)*' in translation_raw or '*(–ø—Ä–µ–¥–ª–æ–≥)*' in translation_raw or '*(—Å–æ—é–∑)*' in translation_raw or '*(–º–µ–∂–¥–æ–º–µ—Ç–∏–µ)*' in translation_raw:
            pos = PartOfSpeech.OTHER

    # –£–±–∏—Ä–∞–µ–º –ø–æ–º–µ—Ç–∫–∏ —Ç–∏–ø–∞ *(Pl.)* –∏–∑ –ª–µ–º–º—ã
    lemma = re.sub(r'\s*\*\([^)]+\)\*', '', lemma).strip()

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ —Å–º—ã—Å–ª—É (–±–∞–∑–æ–≤–∞—è –ª–æ–≥–∏–∫–∞)
    categories = determine_category(lemma, translation)

    return {
        'lemma': lemma,
        'article': article,
        'pos': pos,
        'translation_ru': translation,
        'categories': categories
    }


def determine_category(lemma: str, translation: str) -> list[str]:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å–ª–æ–≤–∞"""
    categories = []

    # –°–ª–æ–≤–∞—Ä—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ ‚Üí –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    keywords = {
        '–µ–¥–∞': ['Brot', 'Butter', 'K√§se', 'Fleisch', 'Ei', 'Kuchen', 'Salat', 'Suppe', 'Essen'],
        '–Ω–∞–ø–∏—Ç–∫–∏': ['Wasser', 'Milch', 'Kaffee', 'Tee', 'Saft', 'Bier', 'Wein'],
        '—Å–µ–º—å—è': ['Mutter', 'Vater', 'Kind', 'Sohn', 'Tochter', 'Familie', 'Eltern', 'Gro√üeltern'],
        '–¥–æ–º': ['Haus', 'Wohnung', 'Zimmer', 'K√ºche', 'Bad', 'T√ºr', 'Fenster'],
        '–º–µ–±–µ–ª—å': ['Tisch', 'Stuhl', 'Bett', 'Schrank', 'Lampe'],
        '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç': ['Auto', 'Bus', 'Zug', 'Fahrrad', 'Flugzeug'],
        '—É—á—ë–±–∞': ['Schule', 'Lehrer', 'Sch√ºler', 'Buch', 'Heft', 'lernen', 'lesen', 'schreiben'],
        '—Ä–∞–±–æ—Ç–∞': ['Arbeit', 'Beruf', 'arbeiten'],
        '–≤—Ä–µ–º—è': ['Tag', 'Jahr', 'Monat', 'Woche', 'Stunde', 'Minute', 'Uhr', 'Zeit'],
        '–ø–æ–≥–æ–¥–∞': ['Wetter', 'Sonne', 'Regen', 'Schnee'],
        '–∂–∏–≤–æ—Ç–Ω—ã–µ': ['Hund', 'Katze', 'Tier', 'Pferd'],
        '–≥–æ—Ä–æ–¥': ['Stadt', 'Stra√üe', 'Park', 'Supermarkt'],
        '–ª—é–¥–∏': ['Mann', 'Frau', 'Freund', 'Freundin', 'Leute'],
    }

    for category, words in keywords.items():
        if lemma in words:
            categories.append(category)

    # –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–µ—Ç ‚Äî —Å—Ç–∞–≤–∏–º –æ–±—â—É—é
    if not categories:
        categories = ['–±–∞–∑–æ–≤—ã–µ —Å–ª–æ–≤–∞']

    return categories


async def import_words():
    """–ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Å–ª–æ–≤–∞ –∏–∑ —Ñ–∞–π–ª–∞ –≤ –ë–î"""
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏–º–ø–æ—Ä—Ç —Å–ª–æ–≤...")

    async with AsyncSessionLocal() as session:
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
        try:
            with open('words_goethe_a1.txt', 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except FileNotFoundError:
            print("‚ùå –§–∞–π–ª words_goethe_a1.txt –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            return

        print(f"üìÑ –ü—Ä–æ—á–∏—Ç–∞–Ω–æ {len(lines)} —Å—Ç—Ä–æ–∫")

        imported_count = 0
        skipped_count = 0

        for line in lines:
            line = line.strip()

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            if not line or '–ù–µ–º–µ—Ü–∫–æ–µ —Å–ª–æ–≤–æ' in line or '–ü–µ—Ä–µ–≤–æ–¥' in line:
                continue

            # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–æ–∫—É
            word_data = await parse_word_line(line)

            if not word_data:
                skipped_count += 1
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–æ–µ —Å–ª–æ–≤–æ
            result = await session.execute(
                select(MasterWord).where(
                    MasterWord.lemma == word_data['lemma'],
                    MasterWord.cefr == CEFRLevel.A1
                )
            )
            existing = result.scalar_one_or_none()

            if existing:
                skipped_count += 1
                continue

            # –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–æ
            word = MasterWord(
                lemma=word_data['lemma'],
                article=word_data['article'],
                pos=word_data['pos'],
                cefr=CEFRLevel.A1,
                translation_ru=word_data['translation_ru'],
                example_de=None,
                example_ru=None,
                plural=None,
                separable_prefix=None
            )

            session.add(word)
            await session.flush()

            # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            for cat_name in word_data['categories']:
                # –ò—â–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                result = await session.execute(
                    select(Category).where(Category.slug == cat_name)
                )
                category = result.scalar_one_or_none()

                if not category:
                    category = Category(
                        slug=cat_name,
                        title_ru=cat_name.capitalize()
                    )
                    session.add(category)
                    await session.flush()

                # –°–≤—è–∑—ã–≤–∞–µ–º —Å–ª–æ–≤–æ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π
                word_cat = WordCategory(
                    word_id=word.id,
                    category_id=category.id,
                    weight=1
                )
                session.add(word_cat)

            imported_count += 1

            if imported_count % 50 == 0:
                print(f"‚úÖ –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {imported_count} —Å–ª–æ–≤...")

        await session.commit()

        print(f"\nüéâ –ò–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à—ë–Ω!")
        print(f"‚úÖ –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ: {imported_count} —Å–ª–æ–≤")
        print(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_count} —Å—Ç—Ä–æ–∫")


if __name__ == "__main__":
    asyncio.run(import_words())